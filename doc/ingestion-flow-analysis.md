# GPT-RAG Ingestion æµç¨‹åˆ†æèˆ‡æ•ˆèƒ½èª¿æ•´æŒ‡å—

## ğŸ“‹ æ¦‚è¦½

æœ¬æ–‡æª”åˆ†æ `gpt-rag-ingestion` æœå‹™çš„å‡½æ•¸å‘¼å«æµç¨‹ï¼ŒåŒ…æ‹¬å„å‡½æ•¸çš„ç”¨é€”ã€è¼¸å…¥è¼¸å‡ºå®šç¾©ã€å‘¼å«é †åºï¼Œä»¥åŠå¯èª¿æ•´çš„æ•ˆèƒ½åƒæ•¸ã€‚

---

## ğŸ”„ ä¸»è¦æµç¨‹ (Main Flow)

### 1. å•Ÿå‹•é †åº

```
main.py (FastAPI lifespan)
    â”‚
    â”œâ”€â”€ _ensure_auth_or_exit()     # é©—è­‰èªè­‰
    â”œâ”€â”€ get_config()               # è¼‰å…¥ App Configuration
    â”œâ”€â”€ Telemetry.configure_monitoring()  # è¨­å®šç›£æ§
    â”œâ”€â”€ scheduler.start()          # å•Ÿå‹•æ’ç¨‹å™¨
    â”‚
    â””â”€â”€ æ’ç¨‹ä»»å‹™ (Cron Jobs)
        â”œâ”€â”€ run_blob_index()       # Blob Storage ç´¢å¼•
        â”œâ”€â”€ run_blob_purge()       # Blob Storage æ¸…ç†
        â”œâ”€â”€ run_sharepoint_index() # SharePoint ç´¢å¼•
        â”œâ”€â”€ run_sharepoint_purge() # SharePoint æ¸…ç†
        â”œâ”€â”€ run_nl2sql_index()     # NL2SQL ç´¢å¼•
        â”œâ”€â”€ run_nl2sql_purge()     # NL2SQL æ¸…ç†
        â””â”€â”€ run_images_purge()     # åœ–ç‰‡æ¸…ç†
```

---

## ğŸ“‚ æ ¸å¿ƒæ¨¡çµ„çµæ§‹

```
gpt-rag-ingestion/
â”œâ”€â”€ main.py                    # FastAPI å…¥å£
â”œâ”€â”€ jobs/                      # æ’ç¨‹ä»»å‹™
â”‚   â”œâ”€â”€ blob_storage_indexer.py   # ğŸ”‘ ä¸»è¦ç´¢å¼•å™¨
â”‚   â”œâ”€â”€ sharepoint_indexer.py
â”‚   â”œâ”€â”€ nl2sql_indexer.py
â”‚   â””â”€â”€ *_purger.py
â”œâ”€â”€ chunking/                  # æ–‡æª”åˆ‡åˆ†
â”‚   â”œâ”€â”€ document_chunking.py      # åˆ‡åˆ†å…¥å£
â”‚   â”œâ”€â”€ chunker_factory.py        # Chunker å·¥å» 
â”‚   â””â”€â”€ chunkers/                 # å„ç¨® Chunker
â”‚       â”œâ”€â”€ base_chunker.py
â”‚       â”œâ”€â”€ doc_analysis_chunker.py   # ğŸ”‘ Document Intelligence
â”‚       â”œâ”€â”€ multimodal_chunker.py     # å¤šæ¨¡æ…‹ (åœ–ç‰‡è™•ç†)
â”‚       â”œâ”€â”€ langchain_chunker.py      # ç´”æ–‡å­—
â”‚       â”œâ”€â”€ spreadsheet_chunker.py    # Excel
â”‚       â””â”€â”€ json_chunker.py           # JSON
â””â”€â”€ tools/                     # Azure æœå‹™å®¢æˆ¶ç«¯
    â”œâ”€â”€ doc_intelligence.py       # Document Intelligence API
    â”œâ”€â”€ aoai.py                   # Azure OpenAI (Embeddings)
    â”œâ”€â”€ aisearch.py               # AI Search
    â””â”€â”€ blob.py                   # Blob Storage
```

---

## ğŸ” Blob Storage Indexer è©³ç´°æµç¨‹

### é¡åˆ¥: `BlobStorageDocumentIndexer`

#### `run()` - ä¸»å…¥å£

```python
async def run(self) -> None
```

| é …ç›® | èªªæ˜ |
|------|------|
| **è¼¸å…¥** | ç„¡ (å¾ App Configuration è®€å–è¨­å®š) |
| **è¼¸å‡º** | ç„¡ (çµæœå¯«å…¥ AI Search å’Œ Log Container) |
| **å‘¼å«é †åº** | è¦‹ä¸‹æ–¹æµç¨‹ |

**åŸ·è¡Œæµç¨‹:**
```
run()
â”‚
â”œâ”€â”€ 1. _ensure_clients()           # åˆå§‹åŒ– Azure å®¢æˆ¶ç«¯
â”‚
â”œâ”€â”€ 2. _load_latest_index_state()  # è¼‰å…¥ç¾æœ‰ç´¢å¼•ç‹€æ…‹ (dedup)
â”‚       â””â”€â”€ è¿”å›: Dict[parent_id, last_modified]
â”‚
â”œâ”€â”€ 3. åˆ—èˆ‰ Blob Container
â”‚       â””â”€â”€ æ¯”å° last_modified æ±ºå®šæ˜¯å¦éœ€é‡æ–°ç´¢å¼•
â”‚
â”œâ”€â”€ 4. _gather_limited()           # ä¸¦è¡Œè™•ç† (max_concurrency)
â”‚       â””â”€â”€ _process_one()         # è™•ç†å–®ä¸€æ–‡ä»¶
â”‚
â””â”€â”€ 5. _write_run_summary()        # å¯«å…¥åŸ·è¡Œæ‘˜è¦
```

#### `_process_one()` - è™•ç†å–®ä¸€æ–‡ä»¶

```python
async def _process_one(
    self,
    blob_name: str,         # Blob è·¯å¾‘
    last_modified: datetime, # æœ€å¾Œä¿®æ”¹æ™‚é–“
    content_type: str,      # MIME é¡å‹
    run_id: str             # åŸ·è¡Œæ‰¹æ¬¡ ID
) -> Dict[str, Any]         # {"status": "success/error", "chunks": int}
```

**åŸ·è¡Œæµç¨‹:**
```
_process_one()
â”‚
â”œâ”€â”€ 1. blob_client.get_blob_properties()  # å–å¾— metadata
â”‚
â”œâ”€â”€ 2. blob_client.download_blob()        # ä¸‹è¼‰æ–‡ä»¶å…§å®¹
â”‚
â”œâ”€â”€ 3. DocumentChunker().chunk_documents(data)  # ğŸ”‘ æ ¸å¿ƒåˆ‡åˆ†
â”‚       â”‚
â”‚       â”œâ”€â”€ ChunkerFactory().get_chunker(data)
â”‚       â”‚       â”‚
â”‚       â”‚       â””â”€â”€ ä¾æ“šå‰¯æª”åé¸æ“‡ Chunker:
â”‚       â”‚           â”œâ”€â”€ pdf/png/jpg... â†’ DocAnalysisChunker / MultimodalChunker
â”‚       â”‚           â”œâ”€â”€ docx/pptx â†’ DocAnalysisChunker (éœ€ DocInt 4.0)
â”‚       â”‚           â”œâ”€â”€ xlsx/xls â†’ SpreadsheetChunker
â”‚       â”‚           â”œâ”€â”€ vtt â†’ TranscriptionChunker
â”‚       â”‚           â”œâ”€â”€ json â†’ JSONChunker
â”‚       â”‚           â””â”€â”€ md/txt/html/py â†’ LangChainChunker
â”‚       â”‚
â”‚       â””â”€â”€ chunker.get_chunks()
â”‚
â”œâ”€â”€ 4. _to_search_doc()                   # è½‰æ›ç‚º Search æ–‡æª”æ ¼å¼
â”‚
â””â”€â”€ 5. _replace_parent_docs()             # ä¸Šå‚³åˆ° AI Search
        â”œâ”€â”€ _delete_parent_docs()         # åˆªé™¤èˆŠ chunks
        â””â”€â”€ _upload_in_batches()          # æ‰¹æ¬¡ä¸Šå‚³æ–° chunks
```

---

## ğŸ“„ Document Chunking è©³ç´°æµç¨‹

### é¡åˆ¥: `DocumentChunker`

```python
def chunk_documents(self, data: dict) -> Tuple[list, list, list]
```

| é …ç›® | èªªæ˜ |
|------|------|
| **è¼¸å…¥** | `data` dict åŒ…å« `documentUrl`, `documentBytes`, `documentContentType`, `fileName` |
| **è¼¸å‡º** | `(chunks, errors, warnings)` |

---

### é¡åˆ¥: `ChunkerFactory`

```python
def get_chunker(self, data: dict) -> BaseChunker
```

**Chunker é¸æ“‡é‚è¼¯:**

| å‰¯æª”å | Chunker | èªªæ˜ |
|--------|---------|------|
| `pdf`, `png`, `jpg`, `jpeg`, `bmp`, `tiff` | `DocAnalysisChunker` æˆ– `MultimodalChunker` | ä¾ MULTIMODAL è¨­å®š |
| `docx`, `pptx` | `DocAnalysisChunker` | éœ€ Document Intelligence 4.0 |
| `xlsx`, `xls` | `SpreadsheetChunker` | Excel å°ˆç”¨ |
| `vtt` | `TranscriptionChunker` | å­—å¹•/è½‰éŒ„æª” |
| `json` | `JSONChunker` | JSON çµæ§‹åŒ–è³‡æ–™ |
| `md`, `txt`, `html`, `py`, `csv`, `xml` | `LangChainChunker` | ç´”æ–‡å­—é¡ |
| `nl2sql` | `NL2SQLChunker` | è‡ªç„¶èªè¨€è½‰ SQL |

---

### é¡åˆ¥: `DocAnalysisChunker` (æ ¸å¿ƒ)

**æµç¨‹:**
```
get_chunks()
â”‚
â”œâ”€â”€ 1. _analyze_document_with_retry()     # å‘¼å« Document Intelligence
â”‚       â””â”€â”€ docint_client.analyze_document_from_bytes()
â”‚           â”‚
â”‚           â”œâ”€â”€ POST /documentintelligence/documentModels/prebuilt-layout:analyze
â”‚           â”‚   Body: {"base64Source": <base64_encoded_bytes>}
â”‚           â”‚
â”‚           â””â”€â”€ Polling loop until status == "succeeded"
â”‚               â””â”€â”€ è¿”å›: {"content": <markdown>, "figures": [...]}
â”‚
â”œâ”€â”€ 2. _number_pagebreaks()               # æ¨™è¨˜é ç¢¼
â”‚
â”œâ”€â”€ 3. _chunk_content()                   # åˆ‡åˆ†å…§å®¹
â”‚       â”‚
â”‚       â”œâ”€â”€ _choose_splitter()
â”‚       â”‚   â”œâ”€â”€ Markdown â†’ MarkdownTextSplitter
â”‚       â”‚   â””â”€â”€ å…¶ä»– â†’ RecursiveCharacterTextSplitter
â”‚       â”‚
â”‚       â””â”€â”€ splitter.split_text(content)
â”‚           â””â”€â”€ yield (chunk_text, token_count)
â”‚
â””â”€â”€ 4. _create_chunk() (for each chunk)   # å»ºç«‹ chunk ç‰©ä»¶
        â”‚
        â””â”€â”€ aoai_client.get_embeddings()  # ğŸ”‘ ç”Ÿæˆå‘é‡
            â””â”€â”€ POST /openai/deployments/{model}/embeddings
```

---

### é¡åˆ¥: `BaseChunker._create_chunk()`

```python
def _create_chunk(
    self,
    chunk_id: int,          # åºè™Ÿ
    content: str,           # å…§å®¹æ–‡å­—
    summary: str = "",      # æ‘˜è¦
    embedding_text: str = "",  # ç”¨æ–¼ç”Ÿæˆå‘é‡çš„æ–‡å­—
    title: str = "",        # æ¨™é¡Œ
    page: int = 0,          # é ç¢¼
    offset: int = 0,        # ä½ç½®åç§»
    related_images: list = None,  # ç›¸é—œåœ–ç‰‡
    related_files: list = None    # ç›¸é—œæ–‡ä»¶
) -> dict
```

**è¼¸å‡ºçµæ§‹:**
```python
{
    "chunk_id": int,
    "url": str,              # åŸå§‹æ–‡ä»¶ URL
    "filepath": str,         # æ–‡ä»¶è·¯å¾‘
    "content": str,          # å…§å®¹ (max 32766 bytes)
    "summary": str,
    "contentVector": list,   # Embedding å‘é‡ (3072 ç¶­)
    "captionVector": list,   # æ¨™é¡Œå‘é‡
    "title": str,
    "page": int,
    "offset": int,
    "length": int,
    "relatedImages": list,
    "relatedFiles": list
}
```

---

## âš™ï¸ å¯èª¿æ•´çš„æ•ˆèƒ½åƒæ•¸

### ğŸ¯ é«˜å½±éŸ¿åƒæ•¸ (å»ºè­°å„ªå…ˆèª¿æ•´)

| åƒæ•¸ | é è¨­å€¼ | èªªæ˜ | æ•ˆèƒ½å½±éŸ¿ |
|------|--------|------|----------|
| **`CHUNKING_NUM_TOKENS`** | `2048` | æ¯å€‹ chunk çš„æœ€å¤§ token æ•¸ | â¬†ï¸ è¼ƒå¤§ = è¼ƒå°‘ chunksã€è¼ƒå°‘ embedding å‘¼å«ã€ä½†æª¢ç´¢ç²¾åº¦å¯èƒ½ä¸‹é™ |
| **`CHUNKING_MIN_CHUNK_SIZE`** | `100` | æœ€å° chunk å¤§å° (å°æ–¼æ­¤å€¼æœƒè¢«è·³é) | éå°æœƒç”¢ç”Ÿç„¡æ„ç¾©çš„ chunks |
| **`TOKEN_OVERLAP`** | `100` | chunk ä¹‹é–“çš„é‡ç–Š token æ•¸ | â¬†ï¸ è¼ƒå¤§ = æ›´å¥½çš„ä¸Šä¸‹æ–‡é€£çºŒæ€§ã€ä½†æ›´å¤šå†—é¤˜ |
| **`INDEXER_MAX_CONCURRENCY`** | `8` | ä¸¦è¡Œè™•ç†æ–‡ä»¶æ•¸ | â¬†ï¸ è¼ƒå¤§ = æ›´å¿«ã€ä½†å¯èƒ½è§¸ç™¼ rate limit |
| **`INDEXER_BATCH_SIZE`** | `500` | AI Search æ‰¹æ¬¡ä¸Šå‚³å¤§å° | Azure å»ºè­° 500-1000 |

### ğŸ“Š Document Intelligence åƒæ•¸

| åƒæ•¸ | é è¨­å€¼ | èªªæ˜ |
|------|--------|------|
| **`DOC_INTELLIGENCE_API_VERSION`** | `2024-11-30` | API ç‰ˆæœ¬ (4.0+ æ”¯æ´ docx/pptx) |
| **`MULTIMODAL`** | `false` | å•Ÿç”¨åœ–ç‰‡è™•ç† (MultimodalChunker) |
| **`MINIMUM_FIGURE_AREA_PERCENTAGE`** | `4.0` | åœ–ç‰‡æœ€å°é¢ç©ç™¾åˆ†æ¯” (ä½æ–¼æ­¤å€¼å¿½ç•¥) |

### ğŸ”¢ Embedding åƒæ•¸

| åƒæ•¸ | é è¨­å€¼ | èªªæ˜ |
|------|--------|------|
| **`EMBEDDINGS_VECTOR_DIMENSIONS`** | `3072` | å‘é‡ç¶­åº¦ (text-embedding-3-large) |
| **`EMBEDDING_DEPLOYMENT_NAME`** | - | Azure OpenAI embedding æ¨¡å‹éƒ¨ç½²åç¨± |

### ğŸ“ Storage åƒæ•¸

| åƒæ•¸ | é è¨­å€¼ | èªªæ˜ |
|------|--------|------|
| **`DOCUMENTS_STORAGE_CONTAINER`** | `documents` | æ–‡æª”ä¾†æº container |
| **`BLOB_PREFIX`** | `""` | åªè™•ç†ç‰¹å®šå‰ç¶´çš„ blobs |
| **`JOBS_LOG_CONTAINER`** | `jobs` | åŸ·è¡Œæ—¥èªŒ container |

---

## ğŸš€ æ•ˆèƒ½èª¿æ•´å»ºè­°

### 1. Chunking ç­–ç•¥å„ªåŒ–

| å ´æ™¯ | å»ºè­°è¨­å®š | åŸå›  |
|------|----------|------|
| **é•·æ–‡æª”ã€éœ€è¦å®Œæ•´ä¸Šä¸‹æ–‡** | `CHUNKING_NUM_TOKENS=4096`, `TOKEN_OVERLAP=200` | æ¸›å°‘ chunk æ•¸é‡ã€ä¿æŒä¸Šä¸‹æ–‡ |
| **çŸ­å•ç­”ã€é«˜ç²¾åº¦éœ€æ±‚** | `CHUNKING_NUM_TOKENS=1024`, `TOKEN_OVERLAP=50` | æ›´ç´°ç²’åº¦ã€æ›´ç²¾ç¢ºçš„æª¢ç´¢ |
| **æ··åˆæ–‡æª”** | ä¿æŒé è¨­ `2048/100` | å¹³è¡¡ç­–ç•¥ |

### 2. ä¸¦è¡Œè™•ç†å„ªåŒ–

```python
# å»ºè­°æ ¹æ“š Document Intelligence å’Œ OpenAI çš„ rate limit èª¿æ•´
INDEXER_MAX_CONCURRENCY = 8    # é è¨­ (å®‰å…¨)
INDEXER_MAX_CONCURRENCY = 16   # é«˜ throughput (éœ€ç›£æ§ 429 éŒ¯èª¤)
INDEXER_MAX_CONCURRENCY = 4    # ä½ throughput (rate limit åš´æ ¼æ™‚)
```

### 3. æ¸›å°‘ API å‘¼å«

| å„ªåŒ–é …ç›® | æ–¹æ³• | ç¯€çœ |
|----------|------|------|
| **è·³éæœªè®Šæ›´æ–‡ä»¶** | ç³»çµ±å·²å…§å»º (æ¯”å° `last_modified`) | Document Intelligence + Embedding è²»ç”¨ |
| **å¢å¤§ chunk å¤§å°** | `CHUNKING_NUM_TOKENS=4096` | Embedding å‘¼å«æ¬¡æ•¸ -50% |
| **åœç”¨å°åœ–ç‰‡** | `MINIMUM_FIGURE_AREA_PERCENTAGE=10` | åœ–ç‰‡è™•ç†è²»ç”¨ |

### 4. æˆæœ¬ vs å“è³ªæ¬Šè¡¡

| å„ªå…ˆç´š | è¨­å®š | æ•ˆæœ |
|--------|------|------|
| **æˆæœ¬å„ªå…ˆ** | å¤§ chunkã€å° overlap | è¼ƒå°‘ API å‘¼å«ã€å¯èƒ½é™ä½æª¢ç´¢ç²¾åº¦ |
| **å“è³ªå„ªå…ˆ** | å° chunkã€å¤§ overlap | æ›´ç²¾ç¢ºæª¢ç´¢ã€è¼ƒé«˜ API æˆæœ¬ |
| **å¹³è¡¡** | `2048/100` (é è¨­) | æ¨è–¦çš„èµ·å§‹é» |

---

## ğŸ”§ èª¿æ•´ç¯„ä¾‹

### ç¯„ä¾‹ 1: å„ªåŒ–é•·æ–‡æª”è™•ç†

```env
# App Configuration è¨­å®š
CHUNKING_NUM_TOKENS=4096
TOKEN_OVERLAP=200
CHUNKING_MIN_CHUNK_SIZE=200
```

### ç¯„ä¾‹ 2: é«˜ throughput æ‰¹æ¬¡è™•ç†

```env
INDEXER_MAX_CONCURRENCY=16
INDEXER_BATCH_SIZE=1000
```

### ç¯„ä¾‹ 3: å•Ÿç”¨å¤šæ¨¡æ…‹åœ–ç‰‡è™•ç†

```env
MULTIMODAL=true
MINIMUM_FIGURE_AREA_PERCENTAGE=5.0
DOCUMENTS_IMAGES_STORAGE_CONTAINER=documents-images
```

---

## ğŸ“Š ç›£æ§æŒ‡æ¨™

å»ºè­°ç›£æ§ä»¥ä¸‹æŒ‡æ¨™ä»¥è©•ä¼°æ•ˆèƒ½:

| æŒ‡æ¨™ | ä¾†æº | æ­£å¸¸ç¯„åœ |
|------|------|----------|
| æ¯æ–‡ä»¶è™•ç†æ™‚é–“ | Application Insights | < 30 ç§’ (ä¸€èˆ¬æ–‡ä»¶) |
| Chunk æ•¸é‡ / æ–‡ä»¶ | Job logs | 10-100 (ä¾æ–‡ä»¶å¤§å°) |
| Embedding API å»¶é² | Application Insights | < 500ms |
| 429 Rate Limit éŒ¯èª¤ | Application Insights | 0 (ç†æƒ³) |
| Document Intelligence è™•ç†æ™‚é–“ | Application Insights | < 60 ç§’ / æ–‡ä»¶ |

---

## ğŸ¢ å¤šç§Ÿæˆ¶ Ingestion é…ç½® (2026-01-20)

### é…ç½®æ–¹å¼

Ingestion é€é App Configuration æŒ‡å®šç›®æ¨™å®¹å™¨å’Œç´¢å¼•ã€‚

**é‡è¦**: é…ç½®é …å¿…é ˆä½¿ç”¨ `gpt-rag` labelï¼

| é…ç½®é … | é è¨­å€¼ | å¤šç§Ÿæˆ¶ç¯„ä¾‹ |
|--------|--------|------------|
| `DOCUMENTS_STORAGE_CONTAINER` | `documents` | `documents-company-a` |
| `SEARCH_RAG_INDEX_NAME` | `ragindex-{token}` | `ragindex-company-a` |

### é…ç½®è®€å–æ™‚æ©Ÿ

- é…ç½®åœ¨**å®¹å™¨å•Ÿå‹•æ™‚**å¾ App Configuration è¼‰å…¥ä¸¦å¿«å–
- ä¿®æ”¹é…ç½®å¾Œ**å¿…é ˆé‡å•Ÿå®¹å™¨**æ‰æœƒç”Ÿæ•ˆ

### æ‰‹å‹•è§¸ç™¼ Ingestion æ­¥é©Ÿ

```powershell
# 1. æ›´æ–° App Configuration (æ³¨æ„ --label gpt-rag)
az appconfig kv set --name appcs-{token} --key "DOCUMENTS_STORAGE_CONTAINER" --value "documents-company-a" --label "gpt-rag" --auth-mode login -y
az appconfig kv set --name appcs-{token} --key "SEARCH_RAG_INDEX_NAME" --value "ragindex-company-a" --label "gpt-rag" --auth-mode login -y

# 2. é‡å•Ÿå®¹å™¨ (å»ºç«‹æ–° Revision)
az containerapp revision copy --name ca-{token}-dataingest --resource-group rg-{name} --cpu 0.5 --memory 1.0Gi

# 3. ç›£æ§åŸ·è¡Œçµæœ
az containerapp logs show --name ca-{token}-dataingest --resource-group rg-{name} --type console --tail 100 | Select-String "RUN-COMPLETE|sourceContainer"
```

### é©—è­‰ Index è³‡æ–™

```powershell
$headers = @{"api-key"="{search-admin-key}"; "Content-Type"="application/json"}
$body = '{"search":"*","top":0,"count":true}'
$r = Invoke-RestMethod -Uri "https://srch-{token}.search.windows.net/indexes/ragindex-company-a/docs/search?api-version=2024-07-01" -Headers $headers -Method POST -Body $body
Write-Host "ragindex-company-a: $($r.'@odata.count') chunks"
```

---

## ï¿½ å¤§æª”æ¡ˆè™•ç†æ•ˆèƒ½æ¸¬è©¦ (2026-01-20)

### æ¸¬è©¦ç’°å¢ƒ

- **Container App**: `ca-{token}-dataingest` revision 0000009
- **CRON**: `*/5 * * * *` (æ¯ 5 åˆ†é˜)
- **APScheduler**: `max_instances=1` (ä¿è­·æ©Ÿåˆ¶ï¼Œé¿å…é‡è¤‡åŸ·è¡Œ)

### æ¸¬è©¦çµæœ

| æª”æ¡ˆ | å¤§å° | è™•ç†æ™‚é–“ | Chunks | ç‹€æ…‹ |
|------|------|----------|--------|------|
| è”¦å±‹æ‹œè¨ª.pptx | 21 MB | 5.87 ç§’ | 5 | âœ… æˆåŠŸ |
| 20241217å°ç£æ²‰æµ¸å¼åŠ‡å ´è¡¨æ¼”.pptx | 25.4 MB | - | - | âœ… æˆåŠŸ |
| 20241219å°ç£æ²‰æµ¸å¼åŠ‡å ´è¡¨æ¼” V2-F.pptx | 27.4 MB | - | - | âœ… æˆåŠŸ |
| å®¤å…§é«˜çˆ¾å¤«ç·´ç¿’å ´20250826.pptx | 31.3 MB | 33.61 ç§’ | 22 | âœ… æˆåŠŸ |
| ä¸­å°æ‹è³£å¸‚å ´åˆ†æ-20250918-F.pptx | 32.5 MB | 16.54 ç§’ | 31 | âœ… æˆåŠŸ |
| é‡ç¸åœ‹åˆä½œå ±å‘Š_20250124.pptx | 37.9 MB | 12.78 ç§’ | 6 | âœ… æˆåŠŸ |
| ä¸–ç•ŒçŸ¥åæ™¯è§€å°èª¿æŸ¥V4.pptx | 41.6 MB | 17.26 ç§’ | 7 | âœ… æˆåŠŸ |
| 202508_å…­å¤§æœƒæ©Ÿå™¨äººå ±å‘ŠV4.pptx | 44.3 MB | 12.62 ç§’ | 14 | âœ… æˆåŠŸ |
| ä¸–ç•Œè‘—åå¤§æ¨“20250314.pptx | **95 MB** | >40 åˆ†é˜ | - | â³ è™•ç†ä¸­ |
| ç«¶æ¥­å•†å ´è¨ªæŸ¥å ±å‘Š_20250506.pptx | **97 MB** | >40 åˆ†é˜ | - | â³ è™•ç†ä¸­ |

### é—œéµç™¼ç¾

1. **21-44 MB æª”æ¡ˆ**: éƒ½èƒ½åœ¨ 5-35 ç§’å…§å®Œæˆè™•ç†
2. **95+ MB æª”æ¡ˆ**: Document Intelligence éœ€è¦éå¸¸é•·çš„è™•ç†æ™‚é–“ (>40 åˆ†é˜)
3. **APScheduler ä¿è­·æ©Ÿåˆ¶**: ç•¶å‰ä¸€å€‹ job ä»åœ¨åŸ·è¡Œæ™‚ï¼Œæ–°çš„ CRON è§¸ç™¼æœƒè¢«**è·³é** (ä¸æœƒé‡ç½®)
4. **è™•ç†æ™‚é–“èˆ‡ chunks æ•¸é‡ä¸æˆæ­£æ¯”**: å–æ±ºæ–¼æ–‡ä»¶å…§å®¹è¤‡é›œåº¦

### è¶…å¤§æª”æ¡ˆè™•ç†å»ºè­°

| æª”æ¡ˆå¤§å° | å»ºè­°è™•ç†æ–¹å¼ |
|----------|--------------|
| < 50 MB | æ­£å¸¸æ”¾å…¥ `documents` å®¹å™¨ |
| 50-100 MB | å¯å˜—è©¦ï¼Œä½†è™•ç†æ™‚é–“å¯èƒ½è¼ƒé•· |
| > 100 MB | å»ºè­°åˆ†å‰²æª”æ¡ˆæˆ–ä½¿ç”¨ `documents-large` å®¹å™¨éš”é›¢ |

### å•é¡Œæª”æ¡ˆé¡å‹

| å•é¡Œé¡å‹ | ç¯„ä¾‹ | è§£æ±ºæ–¹æ¡ˆ |
|----------|------|----------|
| Token è¶…é™ Excel | å±•æ¼”é¤¨å ´åœ°èª¿æŸ¥2025.xlsx (502K tokens) | ç§»è‡³ `documents-large` éš”é›¢ |
| è¶…å¤§ PowerPoint | 95+ MB æª”æ¡ˆ | è€ƒæ…®åˆ†å‰²æˆ–éš”é›¢ |

---

## ï¿½ğŸ“ é—œéµå‡½æ•¸åƒè€ƒè¡¨

| å‡½æ•¸ | ä½ç½® | è¼¸å…¥ | è¼¸å‡º | å¯èª¿åƒæ•¸ |
|------|------|------|------|----------|
| `BlobStorageDocumentIndexer.run()` | jobs/blob_storage_indexer.py | - | - | `max_concurrency`, `batch_size` |
| `DocumentChunker.chunk_documents()` | chunking/document_chunking.py | data dict | (chunks, errors, warnings) | - |
| `ChunkerFactory.get_chunker()` | chunking/chunker_factory.py | data dict | BaseChunker | `MULTIMODAL` |
| `DocAnalysisChunker.get_chunks()` | chunking/chunkers/doc_analysis_chunker.py | - | list[dict] | `CHUNKING_NUM_TOKENS`, `TOKEN_OVERLAP`, `CHUNKING_MIN_CHUNK_SIZE` |
| `BaseChunker._create_chunk()` | chunking/chunkers/base_chunker.py | chunk åƒæ•¸ | dict | `EMBEDDINGS_VECTOR_DIMENSIONS` |
| `DocumentIntelligenceClient.analyze_document_from_bytes()` | tools/doc_intelligence.py | file_bytes, filename | (result, errors) | `DOC_INTELLIGENCE_API_VERSION` |
| `AzureOpenAIClient.get_embeddings()` | tools/aoai.py | text | list[float] | `EMBEDDING_DEPLOYMENT_NAME` |
